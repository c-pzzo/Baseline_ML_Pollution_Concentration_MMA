{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1972762e",
   "metadata": {},
   "source": [
    "# About\n",
    "Hyperparameter optimization is required to get the most out of your machine learning models.\n",
    "\n",
    "Hyperparameters are points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.\n",
    "\n",
    "Parameters are different from hyperparameters. Parameters are learned automatically; hyperparameters are set manually to help guide the learning process.\n",
    "\n",
    "Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b78e61",
   "metadata": {},
   "source": [
    "! https://practicaldatascience.co.uk/machine-learning/how-to-use-model-selection-and-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b72428",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4cf86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User information is ready!\n"
     ]
    }
   ],
   "source": [
    "%run \"/home/cesar/Python_NBs/HDL_Project/HDL_Project/global_fv.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f177fd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cesar/Python_NBs/HDL_Project/Mini HDL/Baseline_ML_Pollution_Concentration_MMA/2_Models'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# Save trained models\n",
    "import joblib\n",
    "\n",
    "# Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "# Hypertuning tools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "# Nonlinear models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# Ensemble models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Clone of time class\n",
    "s = t\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(101)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69844e52",
   "metadata": {},
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27563f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(name, model, space, X, y):\n",
    "    # The searching algorithm includes a “cv” argument that allows:\n",
    "    # a) An integer number of folds to be specified, e.g. 5\n",
    "    #cross_val = 5\n",
    "    # b) A configured cross-validation object.\n",
    "    kfold = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "    # The scoring metric must be maximizing, meaning better models result in larger scores.\n",
    "    scoring_metric = 'neg_mean_squared_error'\n",
    "\n",
    "    # Search for best hyperparameters\n",
    "    grid = RandomizedSearchCV(estimator=model, \n",
    "                              param_distributions=search_space, \n",
    "                              cv=kfold, \n",
    "                              n_iter=100,\n",
    "                              scoring=scoring_metric)\n",
    "\n",
    "    result = grid.fit(X_test, y_test)\n",
    "    \n",
    "    # Save the trained model\n",
    "    filename = 'trained_ml_models_mvi/{}.sav'.format(name)\n",
    "    joblib.dump(result, filename)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d253219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a single model\n",
    "def single_model_evaluation(X_test, y_test, name):\n",
    "    # Load the trained model\n",
    "    filename = 'trained_ml_models_mvi/{}.sav'.format(name)\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    # make predictions\n",
    "    y_prediction = model.predict(X_test)\n",
    "    \n",
    "    metrics = dict()\n",
    "    # evaluate predictions\n",
    "    metrics[\"RMSE\"] = mean_squared_error(y_test, y_prediction, squared=False)\n",
    "    metrics[\"MAE\"] = mean_absolute_error(y_test, y_prediction)\n",
    "    metrics[\"MAPE (%)\"] = mean_absolute_percentage_error(y_test, y_prediction) *100\n",
    "    metrics[\"R^2 (%)\"] = r2_score(y_test, y_prediction) * 100\n",
    "    metrics[\"Max Error\"] = max_error(y_test, y_prediction)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cacf94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_ml_models_mvi/KNN.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6948/3898833134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'trained_ml_models_mvi/{}.sav'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hdl/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_ml_models_mvi/KNN.sav'"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "filename = 'trained_ml_models_mvi/{}.sav'.format(\"KNN\")\n",
    "model = joblib.load(filename)\n",
    "\n",
    "# make predictions\n",
    "y_prediction = model.predict(X_test)\n",
    "\n",
    "metrics = dict()\n",
    "# evaluate predictions\n",
    "metrics[\"RMSE\"] = mean_squared_error(y_test, y_prediction, squared=False)\n",
    "metrics[\"MAE\"] = mean_absolute_error(y_test, y_prediction)\n",
    "metrics[\"MAPE (%)\"] = mean_absolute_percentage_error(y_test, y_prediction) *100\n",
    "metrics[\"R^2 (%)\"] = r2_score(y_test, y_prediction) * 100\n",
    "metrics[\"Max Error\"] = max_error(y_test, y_prediction)    \n",
    "\n",
    "print(y_prediction)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660cd23",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08cdff",
   "metadata": {},
   "source": [
    "## Sample preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_table = \"MVI_sima_station_CE\"\n",
    "target = \"pm25\"\n",
    "\n",
    "# Define columns of interest from sql table\n",
    "#     Select all columns:\n",
    "column = \"datetime, co, no, no2, o3, pm10, pm25, prs, rainf, rh, so2, sr, tout, wdr, wsr\"\n",
    "# We remove NOx because it has high correlation with NO.\n",
    "\n",
    "#column = \"*\"\n",
    "#     Select specific columns:\n",
    "#column = \"datetime, co, no, no2, nox, o3, pm10, pm25, prs, rainf, rh, so2, sr, tout, wdr, wsr \"\n",
    "\n",
    "# Filter data with WHERE command\n",
    "sql_where = \"where datetime >=\\'2021-04-17 23:00:00\\'\"\n",
    "#\"where datetime > \\'2020-04-20\\'\"\n",
    "\n",
    "# Initialize class to create multivariate samples:\n",
    "multi_ts = multivariate_samples(sql_table, target, column, sql_where)\n",
    "\n",
    "# Datasets can't be trained with sample batches by default. So parameter is 1.\n",
    "X, y, _ = multi_ts.samples_creation(1, target)\n",
    "\n",
    "# Training and test datasets are prepared, avoiding shuffling because it is a time series.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:,0,:], y, test_size = 0.30, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_target(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7bd2f",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04d179",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa61ce7",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "RandomizedSearchCV for random search evaluates models for a given hyperparameter vector using cross-validation, hence the “CV” suffix of each class name.\n",
    "\n",
    "It requires two arguments. \n",
    "1. The first is the model that you are optimizing. This is an instance of the model with values of hyperparameters set that you want to optimize. \n",
    "2. The second is the search space. This is defined as a dictionary where the names are the hyperparameter arguments to the model and the values are discrete values or a distribution of values to sample in the case of a random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f606afc",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb794f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = KNeighborsRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0750b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'n_neighbors': list(range(1,10)),\n",
    "    'weights': list(['uniform', 'distance']),\n",
    "    'algorithm': list(['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "    'leaf_size': list(range(15, 45)),\n",
    "    'p': list([1,2]),\n",
    "    'metric': list(['euclidean', 'manhattan','chebyshev', 'minkowski']),\n",
    "    # The search can be made parallel using various if not all of your CPU cores \n",
    "    # We can set it to -1 to automatically use all of the cores in the system.\n",
    "    'n_jobs': list([-1])\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_KNN = hyper_tuning(\"KNN\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "# Get the results\n",
    "print(result_KNN.best_score_)\n",
    "print(\"\")\n",
    "print(result_KNN.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_KNN.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297245af",
   "metadata": {},
   "source": [
    "## Classification and Regression Tree\n",
    "DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b303d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = DecisionTreeRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'criterion': list(['squared_error', 'friedman_mse', 'absolute_error', 'poisson'])\n",
    "    , 'splitter': list(['best', 'random'])\n",
    "    , 'max_depth': list(range(1,10))\n",
    "    , 'min_samples_split': list(range(2,10))\n",
    "    , 'min_samples_leaf': list(range(1,10))\n",
    "    , 'min_weight_fraction_leaf': list(np.linspace(0.0,0.5))\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32313cfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_DTR = hyper_tuning(\"DecisionTrees\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results\n",
    "print(result_DTR.best_score_)\n",
    "print(\"\")\n",
    "print(result_DTR.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_DTR.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b3674",
   "metadata": {},
   "source": [
    "## Support Vector Regression - Polynomial\n",
    "svm.SVR(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc46eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = svm.SVR()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'kernel': list(['poly'])\n",
    "    # `degree` is a parameter used when kernel is set to ‘poly’.\n",
    "    , 'degree': list([0, 2, 3, 4, 5, 6])\n",
    "    # Gamma is a parameter for non linear hyperplanes. \n",
    "    # The higher the gamma value it tries to exactly fit the training data set\n",
    "    , 'gamma' : list([0.1, 1, 10, 100])\n",
    "    # C is the penalty parameter of the error term. \n",
    "    # It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    , 'C': list([0.1, 1, 10, 100, 1000])\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7656f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(False):\n",
    "    t.tic()\n",
    "    result_SVM_poly = hyper_tuning(\"SVR_Poly\", model, search_space, X_train, y_train)\n",
    "    t.toc(restart=True)\n",
    "\n",
    "    # Get the results\n",
    "    print(result_SVM_poly.best_score_)\n",
    "    print(\"\")\n",
    "    print(result_SVM_poly.best_estimator_)\n",
    "    print(\"\")\n",
    "    print(result_SVM_poly.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31d89a",
   "metadata": {},
   "source": [
    "## Support Vector Regression - RBF\n",
    "svm.SVR(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94870cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = svm.SVR()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'kernel': list(['rbf'])\n",
    "    # Gamma is a parameter for non linear hyperplanes. \n",
    "    # The higher the gamma value it tries to exactly fit the training data set\n",
    "    , 'gamma' : list([0.1, 1, 10, 100])\n",
    "    # C is the penalty parameter of the error term. \n",
    "    # It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    , 'C': list([0.1, 1, 10, 100, 1000])\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_SVM_RBF = hyper_tuning(\"SVR_RBF\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results\n",
    "print(result_SVM_RBF.best_score_)\n",
    "print(\"\")\n",
    "print(result_SVM_RBF.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_SVM_RBF.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99691c",
   "metadata": {},
   "source": [
    "## Support Vector Regression - Linear\n",
    "svm.SVR(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = svm.SVR()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c68ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'kernel': list(['linear'])\n",
    "    # Gamma is a parameter for non linear hyperplanes. \n",
    "    # The higher the gamma value it tries to exactly fit the training data set\n",
    "    , 'gamma' : list([0.1, 1, 10, 100])\n",
    "    # C is the penalty parameter of the error term. \n",
    "    # It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    , 'C': list([0.1, 1, 10, 100, 1000])\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_SVM_Linear = hyper_tuning(\"SVR_Linear\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results\n",
    "print(result_SVM_Linear.best_score_)\n",
    "print(\"\")\n",
    "print(result_SVM_Linear.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_SVM_Linear.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639436d6",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = RandomForestRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    # `n_estimators` represents the number of trees in the forest. \n",
    "    # Usually the higher the number of trees the better to learn the data. It is also computationally expensive.\n",
    "    'n_estimators': list([100, 200, 300, 400, 500])\n",
    "    # `max_depth` represents the depth of each tree in the forest. \n",
    "    # The deeper the tree, the more splits it has and it captures more information about the data.\n",
    "    , 'max_depth': list(np.linspace(1, 32, 32, endpoint=True))\n",
    "    # `min_samples_split` represents the minimum number of samples required to split an internal node. \n",
    "    , 'min_samples_split': list([2, 3, 4, 5, 6, 7, 8, 9, 10]) # list(np.linspace(1, 1, 10, endpoint=True))\n",
    "    # `min_samples_leaf` The minimum number of samples required to be at a leaf node.\n",
    "    #, 'min_samples_leafs': list([1, 2, 4])\n",
    "    # `max_features`: Represents the number of features to consider when looking for the best split.\n",
    "    , 'max_features': list(range(1,X_train.shape[1]))\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a3f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_RF = hyper_tuning(\"RandomForest\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results \n",
    "print(result_RF.best_score_)\n",
    "print(\"\")\n",
    "print(result_RF.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_RF.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447f052",
   "metadata": {},
   "source": [
    "## Extra-trees regressor\n",
    "ExtraTreesRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ae84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = ExtraTreesRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebe9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    # `n_estimators` represents the number of trees in the forest. \n",
    "    # Usually the higher the number of trees the better to learn the data. It is also computationally expensive.\n",
    "    'n_estimators': list([1, 2, 4, 8, 16, 32, 64, 100, 200])\n",
    "    , 'criterion': ['squared_error']\n",
    "    # `max_depth` represents the depth of each tree in the forest. \n",
    "    # The deeper the tree, the more splits it has and it captures more information about the data.\n",
    "    , 'max_depth': list(np.linspace(1, 32, 32, endpoint=True))\n",
    "    # `min_samples_split` represents the minimum number of samples required to split an internal node. \n",
    "    , 'min_samples_split': list([2, 3, 4, 5, 6, 7, 8, 9, 10]) # list(np.linspace(1, 1, 10, endpoint=True))\n",
    "    # `min_samples_leaf` The minimum number of samples required to be at a leaf node.\n",
    "    #, 'min_samples_leafs': list(np.linspace(0.1, 0.5, 5, endpoint=True))\n",
    "    # `max_features`: Represents the number of features to consider when looking for the best split.\n",
    "    , 'max_features': list(range(1,X_train.shape[1]))\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a98b35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_ETR = hyper_tuning(\"ExtraTrees\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results\n",
    "print(result_ETR.best_score_)\n",
    "print(\"\")\n",
    "print(result_ETR.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_ETR.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2096e",
   "metadata": {},
   "source": [
    "## XG Boost \n",
    "XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an algorithm\n",
    "model = XGBRegressor()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = [{\n",
    "    'max_depth': [3, 5, 6, 10, 15, 20]\n",
    "    , 'learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
    "    , 'subsample': np.arange(0.5, 1.0, 0.1)\n",
    "    , 'colsample_bytree': np.arange(0.4, 1.0, 0.1)\n",
    "    , 'colsample_bylevel': np.arange(0.4, 1.0, 0.1)\n",
    "    , 'n_estimators': [100, 500, 1000, 1500, 2000]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86644010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t.tic()\n",
    "result_XGB = hyper_tuning(\"XGBoost\", model, search_space, X_train, y_train)\n",
    "t.toc(restart=True)\n",
    "\n",
    "# Get the results\n",
    "print(result_XGB.best_score_)\n",
    "print(\"\")\n",
    "print(result_XGB.best_estimator_)\n",
    "print(\"\")\n",
    "print(result_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ad9bd",
   "metadata": {},
   "source": [
    "# Loading and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f76c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a dict of models {name:object}, returns {name:score}\n",
    "def multiple_model_evaluation(X_test, y_test, models_list):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    for name in models_list:\n",
    "        # evaluate the model\n",
    "        s.tic()\n",
    "        tmp_df = pd.DataFrame(single_model_evaluation(X_test, y_test, name), index=[0])\n",
    "        tmp_df.insert(0, \"Model Name\", name, True)\n",
    "        tmp_df.insert(0, \"Type\", \"ML\", True)\n",
    "        metrics_df = metrics_df.append(tmp_df)\n",
    "        print(\"> {}.\".format(name))\n",
    "        s.toc(restart=True)\n",
    "        \n",
    "    return metrics_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c72ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model list\n",
    "models_list = [\"KNN\", \"DecisionTrees\", \"SVR_RBF\", \"SVR_Linear\", \"RandomForest\", \"ExtraTrees\", \"XGBoost\"]\n",
    "\n",
    "# evaluate models\n",
    "t.tic() #Start timer\n",
    "results = multiple_model_evaluation(X_test, y_test, models_list)\n",
    "t.toc() #Time elapsed since t.tic()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f88f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842f166",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "## Main \n",
    "https://practicaldatascience.co.uk/machine-learning/how-to-use-model-selection-and-hyperparameter-tuning\n",
    "\n",
    "\n",
    "* sklearn.model_selection.RandomizedSearchCV\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html \n",
    "    - https://scikit-learn.org/stable/modules/grid_search.html?highlight=randomsearchcv\n",
    "* sklearn.model_selection.KFold\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "    - https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "\n",
    "\n",
    "## Models\n",
    "* KNN\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric\n",
    "* DecisionTreeRegressor()\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "* pmdarima\n",
    "    - https://towardsdatascience.com/efficient-time-series-using-pythons-pmdarima-library-f6825407b7f0\n",
    "* SVM\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svm%20svr%20kernel%20poly\n",
    "    - https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769\n",
    "* Random Forest\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "    - https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d\n",
    "* XGBoost\n",
    "    - https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
    "    \n",
    "* Gaussian NB\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "    - https://medium.com/analytics-vidhya/how-to-improve-naive-bayes-9fa698e14cba\n",
    "    - https://www.analyticsvidhya.com/blog/2021/01/gaussian-naive-bayes-with-hyperpameter-tuning/\n",
    "    \n",
    "## Metrics\n",
    "* Metrics and scoring: quantifying the quality of predictions\n",
    "    - https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    - https://openclassrooms.com/en/courses/6401081-improve-the-performance-of-a-machine-learning-model/6539936-improve-your-feature-selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
